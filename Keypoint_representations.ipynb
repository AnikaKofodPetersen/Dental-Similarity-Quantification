{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b16967",
   "metadata": {},
   "source": [
    "# Keypoint representation and correspondance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064f6fad-efe2-44f3-87cf-d2e67c969f6f",
   "metadata": {},
   "source": [
    "##### *3D Dental Similarity Quantification in Forensic Odontology Identification - repository* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96924797",
   "metadata": {},
   "source": [
    "This notebook calculates keypoint representation and saves it as json files. Afterwards the landmark correspondance function can be used to evaluate keypoint correspondance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea9925b",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8059da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "import keypoint_detection as kpd   #The \"no print\" version\n",
    "import os\n",
    "import vedo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.preprocessing import normalize\n",
    "import pyshot\n",
    "import time\n",
    "import json\n",
    "\n",
    "method_name = \"temp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CALCULATE SFH #####\n",
    "def calculate_sfh(keypoints,mesh_points,mesh_normals, bins=8, r=1):\n",
    "    \"\"\" Function for calculating sfh for each landmark\"\"\"\n",
    "    \"\"\"\n",
    "    Calculates the Signed Feature Histogram descriptor given keypoints and mesh surface.\n",
    "    \n",
    "    Parameters:\n",
    "        mesh (vedo mesh): A mesh object representing the vertices and faces of the mesh surface.\n",
    "        keypoints (numpy array):A list of arrays of shape (1, 3) representing the keypoints of interest.\n",
    "        radius (float): The radius within which to consider points for the SFH calculations.\n",
    "        bins (int): The number of bins to use for the histogram (default 8).\n",
    "    \n",
    "    Returns:\n",
    "        sfh_hist (numpy array): An dictionary of arrays of shape (bins, bins, bins) representing the sfh histograms.\n",
    "    \"\"\"\n",
    "    \n",
    "    #print(\"Calculating SFH\")\n",
    "\n",
    "    #Initialize full dict\n",
    "    sfh_dict = {}\n",
    "    dict_key = 0\n",
    "    key_count = 0\n",
    "    tree = cKDTree(mesh_points)\n",
    "    \n",
    "    #Calculate sfh for all landmarks\n",
    "    for i, keypoint in enumerate(keypoints):\n",
    "        #print(\"Processing local support: \",round(((i+1)/len(keypoints))*100,2), \"%    \", end=\"\\r\")\n",
    "        \n",
    "        #Initialize matrix for vertex\n",
    "        vert_mat = []\n",
    "        \n",
    "        #Get LRA\n",
    "        key = -800\n",
    "        #key = mesh.closestPoint(keypoint,returnPointId=True) \n",
    "        #LRAp = mesh.normals()[key]\n",
    "        dist, indices = tree.query(keypoint, k=len(mesh_points), distance_upper_bound=r)\n",
    "        indices = indices[~np.isinf(dist)]\n",
    "        LRAp = np.mean(mesh_normals[indices],axis=0)\n",
    "        \n",
    "        #Get tangent plane\n",
    "        tangent_plane = vedo.Plane(pos=keypoint, normal = LRAp)\n",
    "        \n",
    "        #Get local environment to r units\n",
    "        connected = points_in_radius(mesh_points,keypoint,r=r)\n",
    "\n",
    "        #Iterate through local environment\n",
    "        for vert in connected:\n",
    "            if vert != key:\n",
    "                \n",
    "                #Get LRAx\n",
    "                dist, indices = tree.query(mesh_points[vert], k=len(mesh_points), distance_upper_bound=r)\n",
    "                indices = indices[~np.isinf(dist)]\n",
    "                LRAx = np.mean(mesh_normals[indices],axis=0)\n",
    "                #LRAx = mesh.normals()[vert]\n",
    "                \n",
    "                #Get thetax and account for numerical precision\n",
    "                dot = np.dot(LRAp, LRAx)\n",
    "                #dot = np.minimum(1, dot)\n",
    "                #dot = np.maximum(-1, dot)\n",
    "                Thetax = np.arccos(dot)\n",
    "                \n",
    "                #Get D1\n",
    "                if np.dot(LRAp, (mesh_points[vert]-keypoint)) > 0:\n",
    "                    D1 =  1\n",
    "                else:\n",
    "                    D1 = -1\n",
    "                \n",
    "                #Get D2\n",
    "                if np.dot(LRAp, (mesh_points[vert]-keypoint)) < np.dot(LRAx, (mesh_points[vert]-keypoint)):\n",
    "                    D2 = 1\n",
    "                else:\n",
    "                    D2 = -1\n",
    "                \n",
    "                #sfh calculation -> add to matrix\n",
    "                sfh1 = np.linalg.norm(np.cross((mesh_points[vert]-keypoint),LRAp))\n",
    "                sfh2 = D1*np.dot((mesh_points[vert]-keypoint),LRAp)\n",
    "                sfh3 = D2*Thetax\n",
    "                vert_mat.append([sfh1,sfh2,sfh3])\n",
    "                \n",
    "                \n",
    "        # bin into histogram\n",
    "        hist_mat, hist_edges = np.histogramdd(np.array(vert_mat),bins=(bins))   #, density=True\n",
    "                \n",
    "        #Add matrix to sfh_dict\n",
    "        sfh_dict[dict_key] = list(hist_mat.flatten())\n",
    "        dict_key += 1\n",
    "                \n",
    "    return sfh_dict\n",
    "\n",
    "def points_in_radius(mesh,point,r=1):\n",
    "    \"\"\" Function for selecting points within a certain radius\"\"\"\n",
    "    \"\"\"\n",
    "    Input: radius, mesh and start point\n",
    "    Output: list of point IDS\n",
    "    \"\"\"\n",
    "    point = np.array(point)\n",
    "    mesh_points = np.array(mesh)\n",
    "    distances = np.linalg.norm(mesh_points-point,axis=1)\n",
    "    keep_index = np.where(distances < r)[0]\n",
    "    return keep_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_LRF(mesh, keypoint, radius=1):\n",
    "    \"\"\"\n",
    "    Calculate the Local Reference Frame (LRF) for a given mesh surface, keypoint and radius\n",
    "    as described in the Rotational Projection Statistics (RoPS) paper.\n",
    "    \n",
    "    Parameters:\n",
    "    mesh (np.ndarray): A numpy array of shape (n, 3) representing the vertices of a mesh surface.\n",
    "    keypoint (np.ndarray): A numpy array of shape (1, 3) representing the keypoint of interest.\n",
    "    radius (float): A radius within which to consider points for LRF calculation.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: A numpy array of shape (3, 3) representing the Local Reference Frame (LRF)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all points within radius of the keypoint\n",
    "    tree = cKDTree(mesh)\n",
    "    dist, indices = tree.query(keypoint, k=len(mesh), distance_upper_bound=radius)\n",
    "    indices = indices[~np.isinf(dist)]\n",
    "    points_within_radius = mesh[indices]\n",
    "    \n",
    "    #Calculate weighted points according to distance\n",
    "    distances = np.linalg.norm(points_within_radius, axis=1)\n",
    "    weights = 1.0 / (1.0 + distances/radius)\n",
    "    weighted_points = points_within_radius * weights[:, np.newaxis]\n",
    "\n",
    "    # Calculate the covariance matrix of the weighted points within radius\n",
    "    covariance_matrix = np.cov(weighted_points.T)\n",
    "    \n",
    "    # Calculate the eigenvectors and eigenvalues of the covariance matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "    \n",
    "    # Disambiguation step \n",
    "    num_vectors = points_within_radius.shape[0]\n",
    "    positive_count = np.sum(np.sum(np.sign(points_within_radius) > 0, axis=0))\n",
    "    negative_count = num_vectors - positive_count\n",
    "    majority_sign = 1 if positive_count >= negative_count else -1\n",
    "    eigenvectors = eigenvectors*majority_sign\n",
    "    \n",
    "    # Sort the eigenvectors by descending eigenvalues\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    \n",
    "    # Use the first two eigenvectors to define the x and y axes of the LRF\n",
    "    x_axis = sorted_eigenvectors[:, 0]\n",
    "    z_axis = sorted_eigenvectors[:, 1]\n",
    "    \n",
    "    # Take the cross product of the x and y axes to define the z axis of the LRF\n",
    "    y_axis = np.cross(x_axis, z_axis)\n",
    "    \n",
    "    # Construct the LRF as a 3x3 matrix with the axes as columns\n",
    "    LRF = np.column_stack((x_axis, y_axis, z_axis))\n",
    "    \n",
    "    return LRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7979ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CALCULATE ECSAD #####\n",
    "\n",
    "def cart_to_azel(coord):\n",
    "    \"\"\" Function for converting cartesian coordinates\n",
    "    to azimuth, elevation and radial coordinates\n",
    "    \"\"\"\n",
    "    x,y,z = coord\n",
    "    azimuth = np.arctan(x/y)\n",
    "    elevation = np.arctan(y/z)\n",
    "    r = np.sqrt(x**2 + y**2 + z**2)\n",
    "    return np.array([r,azimuth,elevation])\n",
    "\n",
    "def calculate_surface_normal(normal_vectors):\n",
    "    # Construct the coefficient matrix A\n",
    "    A = np.column_stack(normal_vectors)\n",
    "\n",
    "    # Calculate the pseudo-inverse of A using SVD\n",
    "    A_pinv = np.linalg.pinv(A)\n",
    "\n",
    "    # Define the target vector\n",
    "    b = np.ones((len(normal_vectors),))\n",
    "\n",
    "    # Solve for the least squares solution x\n",
    "    x = np.dot(A_pinv, b)\n",
    "\n",
    "    # Normalize the resulting normal vector\n",
    "    surface_normal = x / np.linalg.norm(x)\n",
    "\n",
    "    return surface_normal\n",
    "\n",
    "def calculate_relative_angle(surface_normal, direction_vector):\n",
    "    # Normalize the surface normal vector\n",
    "    surface_normal = surface_normal / np.linalg.norm(surface_normal)\n",
    "\n",
    "    # Normalize the direction vector\n",
    "    direction_vector = direction_vector / np.linalg.norm(direction_vector)\n",
    "\n",
    "    # Calculate the dot product between the surface normal and direction vectors\n",
    "    dot_product = np.dot(surface_normal, direction_vector)\n",
    "\n",
    "    # Calculate the relative angle in radians\n",
    "    relative_angle = np.arccos(dot_product)\n",
    "\n",
    "    # Convert the relative angle to degrees\n",
    "    relative_angle_degrees = np.degrees(relative_angle)\n",
    "\n",
    "    return relative_angle_degrees\n",
    "\n",
    "def calculate_ECSAD(keypoints,mesh,radius=1):\n",
    "    \"\"\" Function for calculating the Equivalent Circumference Surface Angle Descriptors (ECSAD)\n",
    "    as descriptors for keypoints\"\"\"\n",
    "    \"\"\"\n",
    "    Calculates the Equivalent Circumference Surface Angle Descriptor descriptor given keypoints and mesh surface.\n",
    "    \n",
    "    Parameters:\n",
    "        mesh (vedo mesh): A mesh object representing the vertices and faces of the mesh surface.\n",
    "        keypoints (numpy array):A list of arrays of shape (1, 3) representing the keypoints of interest.\n",
    "        radius (float): The radius within which to consider points for the ECSAD calculations.\n",
    "    \n",
    "    Returns:\n",
    "        ecsad_hist (numpy array): An dictionary of arrays of shape (bins, bins, bins) representing the sfh histograms.\n",
    "    \"\"\"\n",
    "    #Initialize mesh and descirptor dict\n",
    "    mesh_points = mesh.points()\n",
    "    mesh_norms = mesh.normals()\n",
    "    ecsad_dict = {}\n",
    "    dict_key = 0\n",
    "    saves = []\n",
    "    #print(\"Calculating ECSAD\")\n",
    "    \n",
    "    #Iterate through keypoints\n",
    "    for key, keypoint in enumerate(keypoints):\n",
    "        print(\"Processing keypoints: \",round(((key+1)/len(keypoints))*100,2), \"%    \", end=\"\\r\")\n",
    "        \n",
    "        bin_idx = []\n",
    "        ecsad = []\n",
    "        \n",
    "        # Calculate LRF\n",
    "        LRF = calculate_LRF(mesh_points, keypoint, radius=radius)\n",
    "        \n",
    "        # Get the points within the LRF neighborhood \n",
    "        neighborhood_mask = np.sum((mesh_points - keypoint)**2, axis=1) < radius**2\n",
    "        points_in_neighborhood = mesh_points[neighborhood_mask]\n",
    "        surf_norm = np.average(mesh_norms[neighborhood_mask])\n",
    "        neighborhood_idx = np.array(list(range(len(mesh_points))))[neighborhood_mask]\n",
    "        \n",
    "        # Move coordinates to origin\n",
    "        new_origin = points_in_neighborhood-keypoint\n",
    "        #transform\n",
    "        points_in_LRF = np.dot(new_origin, LRF.T)\n",
    "        \n",
    "        # Project to 2D circle to eliminate elevation\n",
    "        circle = np.array(new_origin[:,[0,1]])\n",
    "        \n",
    "        # Split into 6 (60*) radial angle-bins\n",
    "        slicenos = np.int32((np.pi + np.arctan2(circle[:, 1], circle[:, 0])) * (6 / (2 * np.pi)))\n",
    "\n",
    "        # Split each angle-bin into 6 bins according to distance\n",
    "        r_len = radius/4\n",
    "        for slicenum in [np.array(0), np.array(1), np.array(2), np.array(3), np.array(4), np.array(5)]:\n",
    "            idx_copy1 = neighborhood_idx.copy()\n",
    "            idx1 = np.argwhere(np.isin(slicenos, slicenum)).ravel()\n",
    "            points_in_slice = circle[idx1]\n",
    "            idx_copy1 = idx_copy1[idx1]\n",
    "            distances = np.linalg.norm(points_in_slice, axis=1)\n",
    "            \n",
    "\n",
    "            for radial_level in [0,1,2,3]:\n",
    "                idx_copy2 = idx_copy1.copy()\n",
    "                idx2 = np.argwhere((distances>=radial_level*r_len)&(distances<=(radial_level+1)*r_len))\n",
    "                idx2 = idx2.flatten()\n",
    "                points_in_r_level = points_in_slice[idx2]\n",
    "                idx_copy2 = idx_copy2[idx2]\n",
    "\n",
    "                \n",
    "                # Split each distance bin according to number of bins needed at this distance\n",
    "                splittings = [(1/(radial_level+1))*split for split in range(0,(radial_level+1))]\n",
    "                splittings.append(1)\n",
    "                splittings += slicenum\n",
    "                bin_val = (np.pi + np.arctan2(points_in_r_level[:, 1], points_in_r_level[:, 0])) * (6 / (2 * np.pi))\n",
    "                \n",
    "                for j in range(len(splittings)-1):\n",
    "                    idx_copy3 = idx_copy2.copy()\n",
    "                    idx5=np.argwhere((bin_val>splittings[j])&(bin_val<splittings[j+1]))\n",
    "                    idx5 = idx5.flatten()\n",
    "                    points_in_bin_level = points_in_r_level[idx5]\n",
    "                    idx_copy3 = idx_copy3[idx5]\n",
    "                    bin_idx.append(idx_copy3)\n",
    "                    \n",
    "                \n",
    "        #Get only from bin\n",
    "        for binn in bin_idx:\n",
    "            if len(binn) != 0:\n",
    "                bin_points = mesh_points[binn]\n",
    "                bin_norms = mesh.normals()[binn]\n",
    "                dir_vects = bin_points-keypoint\n",
    "                rel_angle = [calculate_relative_angle(surf_norm, vec) for vec in dir_vects]\n",
    "                rel_angle = np.average(rel_angle)\n",
    "            else:\n",
    "                rel_angle = None\n",
    "                \n",
    "            ecsad.append(rel_angle)\n",
    "\n",
    "        #Interpolation\n",
    "        #Inner circle\n",
    "        ecsad_copy1 = ecsad.copy()\n",
    "        first_circle = [0,10,20,30,40,50]\n",
    "        second_circle = [1,2,11,12,21,22,31,32,41,42,51,52]\n",
    "        third_circle = [3,4,5,13,14,15,23,24,25,33,34,35,43,44,45,53,54,55]\n",
    "        fourth_circle = [6,7,8,9,16,17,18,19,26,27,28,29,36,37,38,39,\n",
    "                        46,47,48,49,56,57,58,59]\n",
    "        for i,t in enumerate(first_circle):\n",
    "            if ecsad[t] == None:\n",
    "                lower = [0]\n",
    "                middle = []\n",
    "                higher = []\n",
    "                if t+10 < len(ecsad):\n",
    "                    j = t+10\n",
    "                else:\n",
    "                    j = 0\n",
    "                if ecsad[j] != None:\n",
    "                    middle.append(ecsad[j])\n",
    "                if ecsad[t-10] != None:\n",
    "                    middle.append(ecsad[t-10])\n",
    "                    \n",
    "                if t+1 < len(ecsad):\n",
    "                    j=t+1\n",
    "                else:\n",
    "                    j=0\n",
    "                if ecsad[j] != None:\n",
    "                    higher.append(ecsad[j])\n",
    "                if t+2 < len(ecsad):\n",
    "                    j=t+2\n",
    "                else:\n",
    "                    j=(t+2)-len(ecsad)\n",
    "                if ecsad[j] != None:\n",
    "                    higher.append(ecsad[j])\n",
    "                          \n",
    "                new_val = np.average(np.concatenate((lower,middle,higher)).flatten())\n",
    "                ecsad_copy1[t] = new_val\n",
    "                \n",
    "        \n",
    "        #Second circle\n",
    "        ecsad_copy2 = ecsad_copy1.copy()\n",
    "        for i,t in enumerate(second_circle):\n",
    "            if ecsad_copy1[t] == None:\n",
    "                if int(str(t)[-1]) == 2:\n",
    "                    j = t-2\n",
    "                else:\n",
    "                    j = t-1\n",
    "                \n",
    "                if ecsad_copy1[j] != None:\n",
    "                    lower = [ecsad_copy1[j]]\n",
    "                else:\n",
    "                    lower = []\n",
    "                middle = []\n",
    "                higher = []\n",
    "                if  i+1 < len(second_circle):\n",
    "                    j = i+1\n",
    "                else:\n",
    "                    j= 0\n",
    "                if ecsad_copy1[second_circle[j]] != None: \n",
    "                    middle.append(ecsad_copy1[second_circle[j]])\n",
    "                if ecsad_copy1[second_circle[i-1]] != None:\n",
    "                    middle.append(ecsad_copy1[second_circle[i-1]])\n",
    "                if t+2 < len(ecsad_copy1):\n",
    "                    j=t+2\n",
    "                else:\n",
    "                    j=(t+2)-len(ecsad_copy1)\n",
    "                if ecsad_copy1[j] != None:\n",
    "                    higher.append(ecsad_copy1[t+2])\n",
    "                if t+3 < len(ecsad_copy1):\n",
    "                    j=t+3\n",
    "                else:\n",
    "                    j=(t+3)-len(ecsad_copy1)\n",
    "                if ecsad_copy1[j] != None:\n",
    "                    higher.append(ecsad_copy1[j])\n",
    "                    \n",
    "                new_val = np.average(np.concatenate((lower,middle,higher)).flatten())\n",
    "                ecsad_copy2[t] = new_val\n",
    "                \n",
    "        \n",
    "        #third circle\n",
    "        ecsad_copy3 = ecsad_copy2.copy()\n",
    "        for i,t in enumerate(third_circle):\n",
    "        \n",
    "            \n",
    "            if ecsad_copy2[t] == None:\n",
    "                if int(str(t)[-1]) == 5:\n",
    "                    j = t-3\n",
    "                else :\n",
    "                    j = t -2\n",
    "       \n",
    "                if ecsad_copy2[j] != None:\n",
    "                    lower = [ecsad_copy2[j]]\n",
    "                else:\n",
    "                    lower =  []\n",
    "                middle = []\n",
    "                higher = []\n",
    "                if i+1 == len(third_circle):\n",
    "                    jr = third_circle[0]\n",
    "                else:\n",
    "                    jr = third_circle[i+1]\n",
    "                jl = third_circle[i-1]\n",
    "                if ecsad_copy2[jr] != None:\n",
    "                    middle.append(ecsad_copy2[jr])\n",
    "                if ecsad_copy2[jl] != None:\n",
    "                    middle.append(ecsad_copy2[jl])\n",
    "                if t+3 < len(ecsad_copy2):\n",
    "                    j=t+3\n",
    "                else:\n",
    "                    j=(t+3)-len(ecsad_copy2)\n",
    "                if ecsad_copy2[j] != None:\n",
    "                    higher.append(ecsad_copy2[j])\n",
    "                if t+4 < len(ecsad_copy2):\n",
    "                    j=t+4\n",
    "                else:\n",
    "                    j=(t+4)-len(ecsad_copy2)\n",
    "                if ecsad_copy2[j] != None:\n",
    "                    higher.append(ecsad_copy2[j])\n",
    "                   \n",
    "                new_val = np.average(np.concatenate((lower,middle,higher)).flatten())\n",
    "                ecsad_copy3[t] = new_val\n",
    "                \n",
    "                \n",
    "        #fourth circle\n",
    "        ecsad_copy4 = ecsad_copy3.copy()\n",
    "        for i,t in enumerate(fourth_circle):\n",
    "            if ecsad_copy3[t] == None:\n",
    "                if int(str(t)[-1]) == 9:\n",
    "                    j = t-4\n",
    "                else :\n",
    "                    j = t -3\n",
    "       \n",
    "                if ecsad_copy3[j] != None:\n",
    "                    lower = [ecsad_copy3[j]]\n",
    "                else:\n",
    "                    lower =  []\n",
    "                middle = []\n",
    "                higher = []\n",
    "                if i+1 == len(fourth_circle):\n",
    "                    jr = fourth_circle[0]\n",
    "                else:\n",
    "                    jr = fourth_circle[i+1]\n",
    "                jl = fourth_circle[i-1]\n",
    "                if ecsad_copy3[jr] != None:\n",
    "                    middle.append(ecsad_copy3[jr])\n",
    "                if ecsad_copy3[jl] != None:\n",
    "                    middle.append(ecsad_copy3[jl])\n",
    "                   \n",
    "                new_val = np.average(np.concatenate((lower,middle,higher)).flatten())\n",
    "                ecsad_copy4[t] = new_val\n",
    "             \n",
    "                 \n",
    "        \n",
    "        #Summing opposing bins\n",
    "        ecsad_out = []\n",
    "        for i in range(30):\n",
    "            ecsad_out.append(ecsad_copy4[i]+ecsad_copy4[i+30])\n",
    "\n",
    "\n",
    "        ecsad_dict[dict_key]=list(np.array(ecsad_out))\n",
    "        dict_key += 1\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    return ecsad_dict\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### CALCULATE ROPS #####\n",
    "\n",
    "def central_moment(D,m,n):\n",
    "    \"\"\" Central moment of 2D dsitribution matrix D, in order m and n\"\"\"\n",
    "    meanX = np.sum(D, axis=0) @ np.arange(1, D.shape[1]+1)\n",
    "    meanY = np.sum(D, axis=1) @ np.arange(1, D.shape[0]+1)\n",
    "    X, Y = np.meshgrid(np.arange(1, D.shape[1]+1), np.arange(1, D.shape[0]+1))\n",
    "    integrandXY_mn = np.power((X - meanX),m) * np.power((Y - meanY),n) * D\n",
    "    momentXY_mn = np.sum(integrandXY_mn)\n",
    "    return momentXY_mn\n",
    "\n",
    "\n",
    "def shannon_entropy(matrix):\n",
    "    \"\"\"\n",
    "    Calculates the Shannon entropy of a 2D matrix.\n",
    "\n",
    "    Parameters:\n",
    "        matrix (numpy array): A 2D array.\n",
    "\n",
    "    Returns:\n",
    "        entropy (float): The Shannon entropy of the matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize the matrix\n",
    "    matrix = matrix / np.sum(matrix)\n",
    "\n",
    "    # Calculate the entropy\n",
    "    mask = matrix > 0  # Filter out zeros or negative values\n",
    "    entropy = -np.sum(matrix[mask] * np.log2(matrix[mask] + np.finfo(float).eps))\n",
    "\n",
    "    return np.float64(entropy)\n",
    "\n",
    "def calculate_RoPS(mesh, keypoint, LRF, radius=1, num_bins=5, num_angles=3):\n",
    "    \"\"\"\n",
    "    Calculates the Rotational Projection Statistics (RoPS) descriptor for a given keypoint and mesh surface.\n",
    "    \n",
    "    Parameters:\n",
    "        mesh (numpy array): An array of shape (n, 3) representing the vertices of the mesh surface.\n",
    "        keypoint (numpy array): An array of shape (1, 3) representing the keypoint of interest.\n",
    "        LRF (numpy array): An array of shape (3, 3) representing the local reference frame at the keypoint.\n",
    "        radius (float): The radius within which to consider points for the LRF and RoPS calculations.\n",
    "        num_bins (int): The number of bins to use for the distance and elevation axes (default 3).\n",
    "        num_angles (int): The number of angles to use for the azimuth axis (default 15).\n",
    "    \n",
    "    Returns:\n",
    "        RoPS_hist (numpy array): An array of shape (num_bins, num_bins, num_angles) representing the RoPS histogram.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the angle increments\n",
    "    angle_increments = np.linspace(0, 2*np.pi, num_angles+1)[:-1]\n",
    "\n",
    "    \n",
    "    # Initialize the RoPS histogram\n",
    "    RoPS_hist = np.zeros((num_angles, 3 , 15))\n",
    "\n",
    "\n",
    "    ### Get Local Surface\n",
    "\n",
    "    # Get the keypoint in the LRF coordinates \n",
    "    keypoint_in_LRF = np.dot(keypoint - keypoint, LRF.T)\n",
    "\n",
    "    # Get the points within the LRF neighborhood and transform\n",
    "    neighborhood_mask = np.sum((mesh - keypoint)**2, axis=1) < radius**2\n",
    "    points_in_neighborhood = mesh[neighborhood_mask]\n",
    "    points_in_LRF = np.dot(points_in_neighborhood, LRF.T)\n",
    "    \n",
    "    ### Get Rotated surfaces\n",
    "    projections = []\n",
    "    #Define rotation matrices\n",
    "    for theta in angle_increments:\n",
    "        rot_mat_xyz = [[[1,0,0],[0,np.cos(theta),-np.sin(theta)],[0, np.sin(theta), np.cos(theta)]],\n",
    "                      [[np.cos(theta),0 ,np.sin(theta)],[0,1,0],[-np.sin(theta), 0, np.cos(theta)]],\n",
    "                      [[np.cos(theta),-np.sin(theta), 0],[np.sin(theta), np.cos(theta), 0],[0,0,1]]]\n",
    "            \n",
    "        # Rotate the point cloud\n",
    "        rotated_surfaces = np.array([np.array(list(map(np.matmul,[rot_mat_xyz[rot_i]]*len(points_in_LRF),points_in_LRF))) for rot_i in [0,1,2]])\n",
    "\n",
    "        # Project the pointcloud onto the axis\n",
    "        P1 = rotated_surfaces[:,:,[0,1]]\n",
    "        P2 = rotated_surfaces[:,:,[0,2]]\n",
    "        P3 = rotated_surfaces[:,:,[1,2]]\n",
    "        projections.append(P1)\n",
    "        projections.append(P2)\n",
    "        projections.append(P3)\n",
    "\n",
    "    # Get histogram for each projection, for each rotation\n",
    "    hist_mats = np.array([np.histogramdd(np.array(projection_sub), bins=(num_bins))[0] for projection in projections for projection_sub in projection])\n",
    "\n",
    "    # Normalize each histogram in hist_mats (without the risk of complex numbers)\n",
    "    normalized_hist_mats = []\n",
    "    for hist_mat in hist_mats:\n",
    "        total_count = np.sum(hist_mat)\n",
    "        if total_count != 0:\n",
    "            normalized_hist_mat = hist_mat / total_count\n",
    "            normalized_hist_mats.append(normalized_hist_mat)\n",
    "        else:\n",
    "            # Handle the case where total_count is zero to avoid division by zero\n",
    "            normalized_hist_mats.append(hist_mat)\n",
    "\n",
    "    # Convert the list of normalized histograms back to an array\n",
    "    normalized_hist_mats = np.array(normalized_hist_mats)\n",
    "    \n",
    "    \n",
    "    ### Get Projection Statistics\n",
    "    entropies = np.array([list(map(shannon_entropy, hist_mats))]).T\n",
    "    momentums = np.array([[central_moment(D,mom_i,mom_j) for mom_i in [1,2] for mom_j in [1,2]] for D in hist_mats])\n",
    "    descriptors = np.concatenate((entropies,momentums),axis=1)\n",
    "    descriptor = descriptors.flatten()\n",
    "    #descriptor = descriptor / np.linalg.norm(descriptor) #normalize\n",
    "    \n",
    "    return list(descriptor)\n",
    "\n",
    "def RoPS_Estimator(keypoints,mesh, L=5, T=3, r=15):\n",
    "    \"\"\" Function for calculating RoPS for each landmark\n",
    "    from paper: https://link.springer.com/article/10.1007/s11263-013-0627-y?utm_source=getftr&utm_medium=getftr&utm_campaign=getftr_pilot\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Input: landmark coordinates and mesh\n",
    "    Output: RoPS for each landmark\n",
    "    \"\"\"\n",
    "    \n",
    "    #print(\"Calculating RoPS                  \")\n",
    "    mesh = mesh.points()\n",
    "\n",
    "    #Initialize full dict\n",
    "    rops_dict = {}\n",
    "    dict_key = 0\n",
    "    \n",
    "    #Calculate rops for all landmarks\n",
    "    for i, keypoint in enumerate(keypoints):\n",
    "        keypoint = np.reshape(keypoint, (-1, 3))\n",
    "        #print(\"Processing keypoints: \",round(((i+1)/len(keypoints))*100,2), \"%    \", end=\"\\r\")\n",
    "        \n",
    "        \n",
    "        LRF = calculate_LRF(keypoint, mesh, radius=r)\n",
    "        rops = calculate_RoPS(mesh, keypoint, LRF, radius=r, num_bins=L, num_angles=T)\n",
    "        rops_dict[dict_key] = rops\n",
    "        dict_key += 1\n",
    "        \n",
    "    return rops_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdac751",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CALCULATE USC #####\n",
    "def cart_to_azel(coord):\n",
    "    \"\"\" Function for converting cartesian coordinates\n",
    "    to azimuth, elevation and radial coordinates\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Input: list with x, y, and z coordinate in cartesian system\n",
    "    Output: list of radial, azimuth and elevation coordinates\n",
    "    \"\"\"\n",
    "    x,y,z = coord\n",
    "    azimuth = np.arctan(x/y)\n",
    "    elevation = np.arctan(y/z)\n",
    "    r = np.sqrt(x**2 + y**2 + z**2)\n",
    "    return np.array([r,azimuth,elevation])\n",
    "\n",
    "def calculate_USC(keypoints, mesh, radius = 1, n_azi=12, n_radi=15, n_elev=11):\n",
    "    \"\"\" Function for calculating the Unique Shape Context (USC)\n",
    "    as descriptors for keypoints\"\"\"\n",
    "    \"\"\"\n",
    "    Calculates the Unique Shape Context descriptor given keypoints, radius, bins, and mesh surface.\n",
    "    \n",
    "    Parameters:\n",
    "        mesh (vedo mesh): A mesh object representing the vertices and faces of the mesh surface.\n",
    "        keypoints (numpy array):A list of arrays of shape (1, 3) representing the keypoints of interest.\n",
    "        radius (float): The radius within which to consider points for the SFH calculations.\n",
    "        bins (int): The number of bins to use for the respective directions.\n",
    "    \n",
    "    Returns:\n",
    "        ush_hist (numpy array): An dictionary of arrays of shape (bins, bins, bins) representing the sfh histograms.\n",
    "    \"\"\"\n",
    "    #Initialize mesh and descirptor dict\n",
    "    #print(\"Calculating USC descriptors\")\n",
    "    mesh = mesh.points()\n",
    "    usc_dict = {}\n",
    "    dict_key = 0\n",
    "    \n",
    "    #Iterate through keypoints\n",
    "    for key, keypoint in enumerate(keypoints):\n",
    "        #print(\"Processing local support: \",round(((key+1)/len(keypoints))*100,2), \"%    \", end=\"\\r\")\n",
    "        \n",
    "        # Calculate LRF\n",
    "        LRF = calculate_LRF(mesh, keypoint, radius=radius)\n",
    "        \n",
    "        # Get the points within the LRF neighborhood and transform\n",
    "        neighborhood_mask = np.sum((mesh - keypoint)**2, axis=1) < radius**2\n",
    "        points_in_neighborhood = mesh[neighborhood_mask]\n",
    "        points_in_LRF = np.dot(points_in_neighborhood, LRF.T)\n",
    "        \n",
    "        # From cartesian to azimuth, elevation and radial coordinates\n",
    "        azel_coord = np.array([list(map(cart_to_azel, points_in_LRF))])\n",
    "        azel_coord = np.squeeze(azel_coord)\n",
    "        \n",
    "        #Calculate spherical histogram\n",
    "        hist_mat, hist_edges = np.histogramdd(np.array(azel_coord),bins=(n_radi,n_azi,n_elev))   #, density=True\n",
    "        \n",
    "        #Add matrix to usc_dict\n",
    "        usc_dict[dict_key] = list(hist_mat.flatten())\n",
    "        dict_key += 1\n",
    "        \n",
    "        \n",
    "    return usc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac893ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CALCULATE SHOT #####\n",
    "def calculate_SHOT(keypoints,mesh, radius=1):\n",
    "    \"\"\" Function for calculating SHOT descriptor\n",
    "    NOTICE: the keypoints should be given as point idx\n",
    "    Also notice the usage of the pyshot package\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Calculates the  Signature of Histograms of OrienTations descriptor given keypoints and mesh surface.\n",
    "    \n",
    "    Parameters:\n",
    "        mesh (vedo mesh): A mesh object representing the vertices and faces of the mesh surface.\n",
    "        keypoints (numpy array):A list of point indexes representing the keypoints of interest.\n",
    "        radius (float): The radius within which to consider points for the SHOT calculations.\n",
    "    \n",
    "    Returns:\n",
    "        shot_hist (numpy array): An dictionary of arrays representing the sfh histograms.\n",
    "    \"\"\"\n",
    "    #print(\"Calculating Shot descriptors                                  \")\n",
    "    vertices: np.array =  np.array(mesh.points().astype(np.float64))\n",
    "    faces: np.array =  np.array(mesh.faces())\n",
    "\n",
    "    # a np.array of shape (n, n_descr)\n",
    "    shot_descrs: np.array = pyshot.get_descriptors(\n",
    "        vertices,\n",
    "        faces,\n",
    "        radius=radius,\n",
    "        local_rf_radius=radius,\n",
    "        # The following parameters are optional\n",
    "        min_neighbors=3,\n",
    "        n_bins=10,\n",
    "        double_volumes_sectors=True,\n",
    "        use_interpolation=True,\n",
    "        use_normalization=True,\n",
    "        )\n",
    "    keypoint_descrs = shot_descrs[keypoints]\n",
    "    #keypoint_descrs = np.array([list(map(normalize,keypoint_descrs))])  #normalize\n",
    "    keypoint_descrs = dict(enumerate(keypoint_descrs))\n",
    "    keypoint_descrs = {dict_key:list(v) for dict_key, (k, v) in enumerate(keypoint_descrs.items())}\n",
    "\n",
    "    return keypoint_descrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb37ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac28a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_shot_single(path,query,q_mesh,r):\n",
    "    q_rep = calculate_SHOT(query,q_mesh, radius=r)\n",
    "    print(\"Number of keypoints: \",len(q_rep))\n",
    "    with open(path, \"w\") as fp:\n",
    "        json.dump(q_rep , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2bc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_usc_single(path,query,q_mesh,r):\n",
    "    q_rep = calculate_USC(query,q_mesh, radius=r)\n",
    "    print(\"Number of keypoints: \",len(q_rep))\n",
    "    with open(path, \"w\") as fp:\n",
    "        json.dump(q_rep , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2469f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rops_single(path,query,q_mesh,r):\n",
    "    q_rep = RoPS_Estimator(query,q_mesh, r=r)\n",
    "    print(\"Number of keypoints: \",len(q_rep))\n",
    "    with open(path, \"w\") as fp:\n",
    "        json.dump(q_rep , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31db062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ecsad_single(path,query,q_mesh,r):\n",
    "    q_rep = calculate_ECSAD(query,q_mesh, radius=r)\n",
    "    print(\"Number of keypoints: \",len(q_rep))\n",
    "    with open(path, \"w\") as fp:\n",
    "        json.dump(q_rep , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542729da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sfh_single(path,query,q_mesh,r,bins=8):\n",
    "    q_rep = calculate_sfh(query,q_mesh.points(),q_mesh.normals(), bins=bins, r=r)\n",
    "    print(\"Number of keypoints: \",len(q_rep))\n",
    "    with open(path, \"w\") as fp:\n",
    "        json.dump(q_rep , fp) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b907057",
   "metadata": {},
   "source": [
    "## Calculate representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc14db",
   "metadata": {},
   "source": [
    "#### For single methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab68f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 40\n",
    "for method in [\"rops\",\"usc\",\"ecsad\",\"shot\",\"sfh\"]: #[\"rops\",\"usc\",\"ecsad\",\"shot\",\"sfh\"]\n",
    "    print(f\"Working with method {method}\")\n",
    "    inspection = False\n",
    "    if method == \"sfh\":\n",
    "        run_single = run_sfh_single\n",
    "        kp_id = False\n",
    "        kp_pts = True\n",
    "        bins = 8\n",
    "    elif method == \"ecsad\":\n",
    "        run_single = run_ecsad_single\n",
    "        kp_id = False\n",
    "        kp_pts = True\n",
    "    elif method == \"rops\":\n",
    "        run_single = run_rops_single\n",
    "        kp_id = False\n",
    "        kp_pts = True\n",
    "        L = 5\n",
    "        T = 3\n",
    "    elif method == \"usc\":\n",
    "        run_single = run_usc_single\n",
    "        kp_id = False\n",
    "        kp_pts = True\n",
    "    elif method == \"shot\":\n",
    "        run_single = run_shot_single\n",
    "        kp_id = True\n",
    "        kp_pts = False\n",
    "    \n",
    "    \n",
    "    # Get all combinations of dental input\n",
    "    in_path = \"/path/to/nput/path/\"\n",
    "    names = os.listdir(in_path)\n",
    "    inspection = False\n",
    "    \n",
    "    if method not in []:\n",
    "        for r in [2]: #[1,1.5,2]\n",
    "            if method == \"sfh\":\n",
    "                method_name = method +f\"_r{r}_bins{bins}\"\n",
    "            else:\n",
    "                method_name = method +f\"_r{r}\"\n",
    "            out_path = f\"Path/to/output/path/{method_name}/\"\n",
    "            os.mkdir(out_path)\n",
    "    \n",
    "            for kp_num, name in enumerate(names):\n",
    "                print(name)\n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        1/8\", end = \"\\r\")\n",
    "            \n",
    "                # Keypoint detection for full dentition\n",
    "                teeth_full = vedo.load(os.path.join(in_path,name))\n",
    "                kp = kpd.keypoint_detection(teeth_full, res=res, name=f\"{name[4:28]}_full\", returnIdx = kp_id, returnPts=kp_pts,output=out_path, inspection = inspection)\n",
    "                q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_full.json')\n",
    "                run_single(q_path,kp,teeth_full,r)        \n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        2/8\", end = \"\\r\")\n",
    "            \n",
    "                # Keypoint detection for noisy dentition (0.1)\n",
    "                mel = 1/round(len(teeth_full.points())/teeth_full.area(),3)\n",
    "                teeth_n1 = teeth_full.clone()\n",
    "                n1 = 0.1*mel\n",
    "                teeth_n1 = teeth_n1.pointGaussNoise(sigma=n1)\n",
    "                kp_n1 = kpd.keypoint_detection(teeth_n1, res=res, name=f\"{name[4:28]}_full_n1\", returnIdx = kp_id, returnPts=kp_pts,output=out_path, inspection = inspection)\n",
    "                q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_full_n1.json')\n",
    "                run_single(q_path,kp_n1,teeth_n1,r) \n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        3/8\", end = \"\\r\")\n",
    "            \n",
    "                # Keypoint detection for noisy dentition (0.3)            \n",
    "                teeth_n3 = teeth_full.clone()\n",
    "                n3 = 0.3*mel\n",
    "                teeth_n3 = teeth_n1.pointGaussNoise(sigma=n3)\n",
    "                kp_n3 = kpd.keypoint_detection(teeth_n3, res=res, name=f\"{name[4:28]}_full_n3\", returnIdx = kp_id, returnPts=kp_pts,output=out_path, inspection = inspection)\n",
    "                q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_full_n3.json')\n",
    "                run_single(q_path,kp_n3,teeth_n3,r)             \n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        4/8\", end = \"\\r\")\n",
    "            \n",
    "                #keypoint detection for partial 1\n",
    "                teeth_partial1 = teeth_full.clone()\n",
    "                for i, point in enumerate(teeth_full.points()):\n",
    "                    teeth_partial1.points()[i] = point + np.array([100,0,0])\n",
    "                teeth_partial1 = teeth_partial1.cutWithPlane(origin= teeth_partial1.centerOfMass(), normal = (1,0,0))\n",
    "                teeth_partial1.decimate(fraction=0.9 ,method='quadratic')\n",
    "                kp1a = kpd.keypoint_detection(teeth_partial1, res=res, name=f\"{name[4:29]}_partial1\", returnIdx = kp_id, returnPts=kp_pts, output=out_path, inspection = inspection)\n",
    "                q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_part1.json')\n",
    "                run_single(q_path,kp1a,teeth_partial1,r) \n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        5/8\", end = \"\\r\")\n",
    "            \n",
    "                #keypoint detection for partial 2\n",
    "                teeth_partial2 = teeth_full.clone()\n",
    "                for i, point in enumerate(teeth_full.points()):\n",
    "                    teeth_partial2.points()[i] = point + np.array([0,-100,0])\n",
    "                teeth_partial2 = teeth_partial2.cutWithPlane(origin= teeth_partial2.centerOfMass(), normal = (0,1,0))\n",
    "                teeth_partial2.decimate(fraction=0.85 ,method='quadratic')\n",
    "                kp1b = kpd.keypoint_detection(teeth_partial2, res=res, name=f\"{name[4:29]}_partial2\", returnIdx = kp_id,returnPts=kp_pts, output=out_path, inspection = inspection)\n",
    "                q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_part2.json')\n",
    "                run_single(q_path,kp1b,teeth_partial2,r) \n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        6/8\", end = \"\\r\")\n",
    "            \n",
    "                # Keypoint detection for noisy dentition (0.1)\n",
    "                mel = 1/round(len(teeth_partial2.points())/teeth_partial2.area(),3)\n",
    "                teeth_part_n1 = teeth_partial2.clone()\n",
    "                n1 = 0.1*mel\n",
    "                teeth_part_n1 = teeth_part_n1.pointGaussNoise(sigma=n1)\n",
    "                kp1b_n1 = kpd.keypoint_detection(teeth_part_n1, res=res, name=f\"{name[4:28]}_partial2_n1\", returnIdx = kp_id, returnPts=kp_pts,output=out_path, inspection = inspection)\n",
    "                q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_part2_n1.json')\n",
    "                run_single(q_path,kp1b_n1,teeth_part_n1,r) \n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        7/8\", end = \"\\r\")\n",
    "            \n",
    "                # Keypoint detection for noisy dentition (0.1)\n",
    "                mel = 1/round(len(teeth_partial2.points())/teeth_partial2.area(),3)\n",
    "                teeth_part_n3 = teeth_partial2.clone()\n",
    "                n3 = 0.3*mel\n",
    "                teeth_part_n3 = teeth_part_n3.pointGaussNoise(sigma=n3)\n",
    "                kp1b_n3 = kpd.keypoint_detection(teeth_part_n3, res=res, name=f\"{name[4:28]}_partial2_n3\", returnIdx = kp_id, returnPts=kp_pts,output=out_path, inspection = inspection)\n",
    "                q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_part2_n3.json')\n",
    "                run_single(q_path,kp1b_n3,teeth_part_n3,r) \n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        8/8\", end = \"\\r\")\n",
    "            \n",
    "                #keypoint detection for partial 3\n",
    "                teeth_partial3 = teeth_full.clone()\n",
    "                theta=0.3\n",
    "                mat = [[np.cos(theta),-np.sin(theta),0],[np.sin(theta), np.cos(theta), 0],[0,0,1]]\n",
    "                for i, point in enumerate(teeth_full.points()):\n",
    "                    teeth_partial3.points()[i] = np.matmul(mat,point)\n",
    "                teeth_partial3.decimate(fraction=0.80 ,method='quadratic')\n",
    "                kp1c = kpd.keypoint_detection(teeth_partial3, res=res, name=f\"{name[4:29]}_partial3\", returnIdx = kp_id,returnPts=kp_pts, output=out_path, inspection = inspection)\n",
    "                q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_rot.json')\n",
    "                run_single(q_path,kp1c,teeth_partial3,r) \n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        8/8\", end = \"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe385a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24393a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "769c6f63",
   "metadata": {},
   "source": [
    "#### For combination methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829fc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize_Data(data):\n",
    "    data = np.array(data)\n",
    "    norm_data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "    return list(norm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ba802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sfh_single(path,query,q_mesh,r,bins=6):\n",
    "    q_rep = calculate_sfh(query,q_mesh.points(),q_mesh.normals(), bins=bins, r=r)\n",
    "    print(\"Number of keypoints: \",len(q_rep))\n",
    "    return q_rep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d46433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ecsad_single(path,query,q_mesh,r):\n",
    "    q_rep = calculate_ECSAD(query,q_mesh, radius=r)\n",
    "    return q_rep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a39e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rops_single(path,query,q_mesh,r):\n",
    "    q_rep = RoPS_Estimator(query,q_mesh, r=r)\n",
    "    print(\"Number of keypoints: \",len(q_rep))\n",
    "    return q_rep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_usc_single(path,query,q_mesh,r):\n",
    "    q_rep = calculate_USC(query,q_mesh, radius=r)\n",
    "    print(\"Number of keypoints: \",len(q_rep))\n",
    "    return q_rep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd62f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_shot_single(path,query,q_mesh,r):\n",
    "    q_rep = calculate_SHOT(query,q_mesh, radius=r)\n",
    "    print(\"Number of keypoints: \",len(q_rep))\n",
    "    return q_rep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 30\n",
    "for method in [[\"sfh\",\"shot\"], [\"sfh\",\"ecsad\"]]: #[\"rops\",\"usc\",\"ecsad\",\"shot\",\"sfh\"] , #[\"sfh\",\"shot\"], [\"sfh\",\"ecsad\"],[\"sfh\",\"usc\"],[\"shot\",\"usc\"],[\"shot\",\"ecsad\"],[\"ecsad\",\"usc\"]\n",
    "    print(f\"Working with method {method}\")\n",
    "    inspection = False\n",
    "    if method[0] == \"sfh\":\n",
    "        run_single1 = run_sfh_single\n",
    "        kp_id1 = False\n",
    "        kp_pts1 = True\n",
    "        bins = 6\n",
    "    elif method[0] == \"ecsad\":\n",
    "        run_single1 = run_ecsad_single\n",
    "        kp_id1 = False\n",
    "        kp_pts1 = True\n",
    "    elif method[0] == \"rops\":\n",
    "        run_single1 = run_rops_single\n",
    "        kp_id1 = False\n",
    "        kp_pts1 = True\n",
    "        L = 5\n",
    "        T = 3\n",
    "    elif method[0] == \"usc\":\n",
    "        run_single1 = run_usc_single\n",
    "        kp_id1 = False\n",
    "        kp_pts1 = True\n",
    "    elif method[0] == \"shot\":\n",
    "        run_single1 = run_shot_single\n",
    "        kp_id1 = True\n",
    "        kp_pts1 = False\n",
    "        \n",
    "    if method[1] == \"sfh\":\n",
    "        run_single2 = run_sfh_single\n",
    "        kp_id2 = False\n",
    "        kp_pts2 = True\n",
    "        bins = 6\n",
    "    elif method[1] == \"ecsad\":\n",
    "        run_single2 = run_ecsad_single\n",
    "        kp_id2 = False\n",
    "        kp_pts2 = True\n",
    "    elif method[1] == \"rops\":\n",
    "        run_single2 = run_rops_single\n",
    "        kp_id2 = False\n",
    "        kp_pts2 = True\n",
    "        L = 5\n",
    "        T = 3\n",
    "    elif method[1] == \"usc\":\n",
    "        run_single2 = run_usc_single\n",
    "        kp_id2 = False\n",
    "        kp_pts2 = True\n",
    "    elif method[1] == \"shot\":\n",
    "        run_single2 = run_shot_single\n",
    "        kp_id2 = True\n",
    "        kp_pts2 = False\n",
    "    \n",
    "    \n",
    "    # Get all combinations of dental input\n",
    "    in_path = \"/path/to/input/path/\"\n",
    "    names = os.listdir(in_path)\n",
    "    inspection = False\n",
    "    \n",
    "    if method not in []:\n",
    "        for r in [2]: \n",
    "            method_name =  method[0] +\"_\"+method[1] + \"_\"\n",
    "            if \"sfh\" in method:\n",
    "                method_name += f\"r{r}_bins{bins}\"\n",
    "            else:\n",
    "                method_name += f\"r{r}\"\n",
    "            out_path = f\"/path/to/output/path/{method_name}/\"\n",
    "            os.mkdir(out_path)\n",
    "    \n",
    "            for kp_num, name in enumerate(names):\n",
    "                print(name)\n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        1/8\", end = \"\\r\")\n",
    "            \n",
    "                # Keypoint detection for full dentition\n",
    "                teeth_full = vedo.load(os.path.join(in_path,name))\n",
    "                if kp_id1 == kp_id2:\n",
    "                    kp_id = kp_id1\n",
    "                    kp_pts = kp_pts1\n",
    "                    kp = kpd.keypoint_detection(teeth_full, res=res, name=f\"{name[4:28]}_full\", returnIdx = kp_id, returnPts=kp_pts,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_full.json')\n",
    "                    q1 = run_single1(q_path,kp,teeth_full,r)\n",
    "                    q2 = run_single2(q_path,kp,teeth_full,r)\n",
    "                else:\n",
    "                    kp1 = kpd.keypoint_detection(teeth_full, res=res, name=f\"{name[4:28]}_full\", returnIdx = kp_id1, returnPts=kp_pts1,output=out_path, inspection = inspection)\n",
    "                    kp2 = kpd.keypoint_detection(teeth_full, res=res, name=f\"{name[4:28]}_full\", returnIdx = kp_id2, returnPts=kp_pts2,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_full.json')\n",
    "                    q1 = run_single1(q_path,kp1,teeth_full,r)\n",
    "                    q2 = run_single2(q_path,kp2,teeth_full,r)\n",
    "                q3 = {}\n",
    "                for k,v in q1.items():\n",
    "                    q3[k] = list(np.concatenate([Normalize_Data(v),Normalize_Data(q2[k])]))\n",
    "                with open(q_path, \"w\") as fp:\n",
    "                    json.dump(q3 , fp)\n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        2/8\", end = \"\\r\")\n",
    "            \n",
    "                # Keypoint detection for noisy dentition (0.1)\n",
    "                mel = 1/round(len(teeth_full.points())/teeth_full.area(),3)\n",
    "                teeth_n1 = teeth_full.clone()\n",
    "                n1 = 0.1*mel\n",
    "                teeth_n1 = teeth_n1.pointGaussNoise(sigma=n1)\n",
    "                if kp_id1 == kp_id2:\n",
    "                    kp_id = kp_id1\n",
    "                    kp_pts = kp_pts1\n",
    "                    kp = kpd.keypoint_detection(teeth_n1, res=res, name=f\"{name[4:28]}_full_n1\", returnIdx = kp_id, returnPts=kp_pts,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_full_n1.json')\n",
    "                    q1 = run_single1(q_path,kp,teeth_full,r)\n",
    "                    q2 = run_single2(q_path,kp,teeth_full,r)\n",
    "                else:\n",
    "                    kp1 = kpd.keypoint_detection(teeth_n1, res=res, name=f\"{name[4:28]}_full_n1\", returnIdx = kp_id1, returnPts=kp_pts1,output=out_path, inspection = inspection)\n",
    "                    kp2 = kpd.keypoint_detection(teeth_n1, res=res, name=f\"{name[4:28]}_full_n1\", returnIdx = kp_id2, returnPts=kp_pts2,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_full_n1.json')\n",
    "                    q1 = run_single1(q_path,kp1,teeth_n1,r)\n",
    "                    q2 = run_single2(q_path,kp2,teeth_n1,r)\n",
    "                q3 = {}\n",
    "                for k,v in q1.items():\n",
    "                    q3[k] = list(np.concatenate([Normalize_Data(v),Normalize_Data(q2[k])]))\n",
    "                with open(q_path, \"w\") as fp:\n",
    "                    json.dump(q3 , fp)\n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        3/8\", end = \"\\r\")\n",
    "            \n",
    "                # Keypoint detection for noisy dentition (0.3)            \n",
    "                teeth_n3 = teeth_full.clone()\n",
    "                n3 = 0.3*mel\n",
    "                teeth_n3 = teeth_n1.pointGaussNoise(sigma=n3)\n",
    "                if kp_id1 == kp_id2:\n",
    "                    kp_id = kp_id1\n",
    "                    kp_pts = kp_pts1\n",
    "                    kp = kpd.keypoint_detection(teeth_n3, res=res, name=f\"{name[4:28]}_full_n3\", returnIdx = kp_id, returnPts=kp_pts,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_full_n3.json')\n",
    "                    q1 = run_single1(q_path,kp,teeth_n3,r)\n",
    "                    q2 = run_single2(q_path,kp,teeth_n3,r)\n",
    "                else:\n",
    "                    kp1 = kpd.keypoint_detection(teeth_n3, res=res, name=f\"{name[4:28]}_full_n3\", returnIdx = kp_id1, returnPts=kp_pts1,output=out_path, inspection = inspection)\n",
    "                    kp2 = kpd.keypoint_detection(teeth_n3, res=res, name=f\"{name[4:28]}_full_n3\", returnIdx = kp_id2, returnPts=kp_pts2,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_full_n3.json')\n",
    "                    q1 = run_single1(q_path,kp1,teeth_n3,r)\n",
    "                    q2 = run_single2(q_path,kp2,teeth_n3,r)\n",
    "                q3 = {}\n",
    "                for k,v in q1.items():\n",
    "                    q3[k] = list(np.concatenate([Normalize_Data(v),Normalize_Data(q2[k])]))\n",
    "                with open(q_path, \"w\") as fp:\n",
    "                    json.dump(q3 , fp)             \n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        4/8\", end = \"\\r\")\n",
    "            \n",
    "                #keypoint detection for partial 1\n",
    "                teeth_partial1 = teeth_full.clone()\n",
    "                for i, point in enumerate(teeth_full.points()):\n",
    "                    teeth_partial1.points()[i] = point + np.array([100,0,0])\n",
    "                teeth_partial1 = teeth_partial1.cutWithPlane(origin= teeth_partial1.centerOfMass(), normal = (1,0,0))\n",
    "                teeth_partial1.decimate(fraction=0.9 ,method='quadratic')\n",
    "                if kp_id1 == kp_id2:\n",
    "                    kp_id = kp_id1\n",
    "                    kp_pts = kp_pts1\n",
    "                    kp = kpd.keypoint_detection(teeth_partial1, res=res, name=f\"{name[4:28]}_part1\", returnIdx = kp_id, returnPts=kp_pts,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_part1.json')\n",
    "                    q1 = run_single1(q_path,kp,teeth_partial1,r)\n",
    "                    q2 = run_single2(q_path,kp,teeth_partial1,r)\n",
    "                else:\n",
    "                    kp1 = kpd.keypoint_detection(teeth_partial1, res=res, name=f\"{name[4:28]}_part1\", returnIdx = kp_id1, returnPts=kp_pts1,output=out_path, inspection = inspection)\n",
    "                    kp2 = kpd.keypoint_detection(teeth_partial1, res=res, name=f\"{name[4:28]}_part1\", returnIdx = kp_id2, returnPts=kp_pts2,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_part1.json')\n",
    "                    q1 = run_single1(q_path,kp1,teeth_partial1,r)\n",
    "                    q2 = run_single2(q_path,kp2,teeth_partial1,r)\n",
    "                q3 = {}\n",
    "                for k,v in q1.items():\n",
    "                    q3[k] = list(np.concatenate([Normalize_Data(v),Normalize_Data(q2[k])]))\n",
    "                with open(q_path, \"w\") as fp:\n",
    "                    json.dump(q3 , fp)\n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        5/8\", end = \"\\r\")\n",
    "            \n",
    "                #keypoint detection for partial 2\n",
    "                teeth_partial2 = teeth_full.clone()\n",
    "                for i, point in enumerate(teeth_full.points()):\n",
    "                    teeth_partial2.points()[i] = point + np.array([0,-100,0])\n",
    "                teeth_partial2 = teeth_partial2.cutWithPlane(origin= teeth_partial2.centerOfMass(), normal = (0,1,0))\n",
    "                teeth_partial2.decimate(fraction=0.85 ,method='quadratic')\n",
    "                if kp_id1 == kp_id2:\n",
    "                    kp_id = kp_id1\n",
    "                    kp_pts = kp_pts1\n",
    "                    kp = kpd.keypoint_detection(teeth_partial2, res=res, name=f\"{name[4:28]}_part2\", returnIdx = kp_id, returnPts=kp_pts,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_part2.json')\n",
    "                    q1 = run_single1(q_path,kp,teeth_partial2,r)\n",
    "                    q2 = run_single2(q_path,kp,teeth_partial2,r)\n",
    "                else:\n",
    "                    kp1 = kpd.keypoint_detection(teeth_partial2, res=res, name=f\"{name[4:28]}_part2\", returnIdx = kp_id1, returnPts=kp_pts1,output=out_path, inspection = inspection)\n",
    "                    kp2 = kpd.keypoint_detection(teeth_partial2, res=res, name=f\"{name[4:28]}_part2\", returnIdx = kp_id2, returnPts=kp_pts2,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_part2.json')\n",
    "                    q1 = run_single1(q_path,kp1,teeth_partial2,r)\n",
    "                    q2 = run_single2(q_path,kp2,teeth_partial2,r)\n",
    "                q3 = {}\n",
    "                for k,v in q1.items():\n",
    "                    q3[k] = list(np.concatenate([Normalize_Data(v),Normalize_Data(q2[k])]))\n",
    "                with open(q_path, \"w\") as fp:\n",
    "                    json.dump(q3 , fp)\n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        6/8\", end = \"\\r\")\n",
    "            \n",
    "                # Keypoint detection for noisy dentition (0.1)\n",
    "                mel = 1/round(len(teeth_partial2.points())/teeth_partial2.area(),3)\n",
    "                teeth_part_n1 = teeth_partial2.clone()\n",
    "                n1 = 0.1*mel\n",
    "                teeth_part_n1 = teeth_part_n1.pointGaussNoise(sigma=n1)\n",
    "                if kp_id1 == kp_id2:\n",
    "                    kp_id = kp_id1\n",
    "                    kp_pts = kp_pts1\n",
    "                    kp = kpd.keypoint_detection(teeth_part_n1, res=res, name=f\"{name[4:28]}_part_n1\", returnIdx = kp_id, returnPts=kp_pts,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_part_n1.json')\n",
    "                    q1 = run_single1(q_path,kp,teeth_part_n1,r)\n",
    "                    q2 = run_single2(q_path,kp,teeth_part_n1,r)\n",
    "                else:\n",
    "                    kp1 = kpd.keypoint_detection(teeth_part_n1, res=res, name=f\"{name[4:28]}_part_n1\", returnIdx = kp_id1, returnPts=kp_pts1,output=out_path, inspection = inspection)\n",
    "                    kp2 = kpd.keypoint_detection(teeth_part_n1, res=res, name=f\"{name[4:28]}_part_n1\", returnIdx = kp_id2, returnPts=kp_pts2,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_part_n1.json')\n",
    "                    q1 = run_single1(q_path,kp1,teeth_part_n1,r)\n",
    "                    q2 = run_single2(q_path,kp2,teeth_part_n1,r)\n",
    "                q3 = {}\n",
    "                for k,v in q1.items():\n",
    "                    q3[k] = list(np.concatenate([Normalize_Data(v),Normalize_Data(q2[k])]))\n",
    "                with open(q_path, \"w\") as fp:\n",
    "                    json.dump(q3 , fp)\n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        7/8\", end = \"\\r\")\n",
    "            \n",
    "                # Keypoint detection for noisy dentition (0.1)\n",
    "                mel = 1/round(len(teeth_partial2.points())/teeth_partial2.area(),3)\n",
    "                teeth_part_n3 = teeth_partial2.clone()\n",
    "                n3 = 0.3*mel\n",
    "                teeth_part_n3 = teeth_part_n3.pointGaussNoise(sigma=n3)\n",
    "                if kp_id1 == kp_id2:\n",
    "                    kp_id = kp_id1\n",
    "                    kp_pts = kp_pts1\n",
    "                    kp = kpd.keypoint_detection(teeth_part_n3, res=res, name=f\"{name[4:28]}_part_n3\", returnIdx = kp_id, returnPts=kp_pts,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_part_n3.json')\n",
    "                    q1 = run_single1(q_path,kp,teeth_part_n3,r)\n",
    "                    q2 = run_single2(q_path,kp,teeth_part_n3,r)\n",
    "                else:\n",
    "                    kp1 = kpd.keypoint_detection(teeth_part_n3, res=res, name=f\"{name[4:28]}_part_n3\", returnIdx = kp_id1, returnPts=kp_pts1,output=out_path, inspection = inspection)\n",
    "                    kp2 = kpd.keypoint_detection(teeth_part_n3, res=res, name=f\"{name[4:28]}_part_n3\", returnIdx = kp_id2, returnPts=kp_pts2,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_part_n3.json')\n",
    "                    q1 = run_single1(q_path,kp1,teeth_part_n3,r)\n",
    "                    q2 = run_single2(q_path,kp2,teeth_part_n3,r)\n",
    "                q3 = {}\n",
    "                for k,v in q1.items():\n",
    "                    q3[k] = list(np.concatenate([Normalize_Data(v),Normalize_Data(q2[k])]))\n",
    "                with open(q_path, \"w\") as fp:\n",
    "                    json.dump(q3 , fp)\n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        8/8\", end = \"\\r\")\n",
    "            \n",
    "                #keypoint detection for partial 3\n",
    "                teeth_partial3 = teeth_full.clone()\n",
    "                theta=0.3\n",
    "                mat = [[np.cos(theta),-np.sin(theta),0],[np.sin(theta), np.cos(theta), 0],[0,0,1]]\n",
    "                for i, point in enumerate(teeth_full.points()):\n",
    "                    teeth_partial3.points()[i] = np.matmul(mat,point)\n",
    "                teeth_partial3.decimate(fraction=0.80 ,method='quadratic')\n",
    "                if kp_id1 == kp_id2:\n",
    "                    kp_id = kp_id1\n",
    "                    kp_pts = kp_pts1\n",
    "                    kp = kpd.keypoint_detection(teeth_partial3, res=res, name=f\"{name[4:28]}_rot\", returnIdx = kp_id, returnPts=kp_pts,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_rot.json')\n",
    "                    q1 = run_single1(q_path,kp,teeth_partial3,r)\n",
    "                    q2 = run_single2(q_path,kp,teeth_partial3,r)\n",
    "                else:\n",
    "                    kp1 = kpd.keypoint_detection(teeth_partial3, res=res, name=f\"{name[4:28]}_rot\", returnIdx = kp_id1, returnPts=kp_pts1,output=out_path, inspection = inspection)\n",
    "                    kp2 = kpd.keypoint_detection(teeth_partial3, res=res, name=f\"{name[4:28]}_rot\", returnIdx = kp_id2, returnPts=kp_pts2,output=out_path, inspection = inspection)\n",
    "                    q_path = os.path.join(out_path,f'{name.split(\".\")[0]}_rot.json')\n",
    "                    q1 = run_single1(q_path,kp1,teeth_partial3,r)\n",
    "                    q2 = run_single2(q_path,kp2,teeth_partial3,r)\n",
    "                q3 = {}\n",
    "                for k,v in q1.items():\n",
    "                    q3[k] = list(np.concatenate([Normalize_Data(v),Normalize_Data(q2[k])]))\n",
    "                with open(q_path, \"w\") as fp:\n",
    "                    json.dump(q3 , fp)\n",
    "                print(f\"Keypoint detection: {round((kp_num/len(names))*100,2)}%        8/8\", end = \"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e724b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c85741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08376f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fc6d382",
   "metadata": {},
   "source": [
    "## Landmark correspondance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496fb226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def convert_to_array(dictionary):\n",
    "    '''Converts lists of values in a dictionary to numpy arrays'''\n",
    "    return {k:np.array(v) for k, v in dictionary.items()}\n",
    "def landmark_correspondence(sfh_dict1, sfh_dict2, method=\"ratio\"):\n",
    "    \"\"\" Function for finding landmark correspondence\"\"\"\n",
    "    \"\"\"\n",
    "    Input: two landmark dicts with flat matrices as values\n",
    "    Output: landmark key correspondence\"\"\"\n",
    "    \n",
    "    #Initialize\n",
    "    distance_ratios = []\n",
    "    warning = False\n",
    "    \n",
    "    #Get size order\n",
    "    if len(sfh_dict1) < len(sfh_dict2):\n",
    "        shortest = sfh_dict1\n",
    "        longest = sfh_dict2\n",
    "    else:\n",
    "        shortest = sfh_dict2\n",
    "        longest = sfh_dict1\n",
    "    \n",
    "    #Iterate through shortest dict\n",
    "    for i, (key1, value1) in enumerate(shortest.items()):\n",
    "        #print(\"Making landmark correspondence: \",round(((i+1)/len(shortest))*100,2), \"%    \", end=\"\\r\")\n",
    "        dist_list = []\n",
    "        \n",
    "        #Get distance to all in longest dict\n",
    "        dist_list = np.linalg.norm((list(longest.values())-value1),axis=1)\n",
    "\n",
    "        #Sort landmarks according to similarity \n",
    "        dists_sorted, idx_matches = zip(*sorted(zip(dist_list,[idx for idx in range(len(dist_list))])))\n",
    "        \n",
    "        #Ratio between best match and second best match\n",
    "        if method == \"ratio\":\n",
    "            if dists_sorted[1] != 0 and dists_sorted[1] != dists_sorted[0]:\n",
    "                L2_ratio = dists_sorted[0]/dists_sorted[1]\n",
    "                distance_ratios.append(L2_ratio)\n",
    "            else:\n",
    "                L2_ratio = 1\n",
    "                distance_ratios.append(L2_ratio)\n",
    "        else:\n",
    "            distance_ratios.append(dists_sorted[0])\n",
    "\n",
    "\n",
    "    #Score\n",
    "    score = np.subtract(*np.percentile(distance_ratios, [75, 25]))\n",
    "    #print(\"Final Score: \",score)\n",
    "    \n",
    "    #Sanity check\n",
    "    if min(distance_ratios) < 0:\n",
    "        print(\"\\nWARNING! distance ratio below 0\")\n",
    "        warning = True\n",
    "\n",
    "\n",
    "    return distance_ratios, score, warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for method in [\"shot_ecsad_r2\",\"shot_usc_r2\",\"shot_sfh_r2\",\"shot_rops_r2\",\"ecsad_usc_r2\",\"ecsad_sfh_r2\",\"ecsad_rops_r2\",\"usc_sfh_r2\",\"usc_rops_r2\",\"sfh_rops_r2\"]: #\n",
    "    print(f\"Working with method {method}\")\n",
    "    \n",
    "    # Get all combinations of dental input\n",
    "    in_path = \"Path/to/inpath/\"\n",
    "    names = os.listdir(in_path)\n",
    "    inspection = False\n",
    "    kp_id = False\n",
    "    kp_pts = True\n",
    "    \n",
    "    for r in [1]: #[1,1.5,2]\n",
    "        if method == \"sfh\":\n",
    "            bins = 10 \n",
    "            method_name = f\"sfh_r{r}_bins{bins}\"\n",
    "        elif method == \"ecsad\":\n",
    "            method_name = f\"ecsad_r{r}\"\n",
    "        elif method == \"rops\":\n",
    "            method_name = f\"rops_r{r}\"\n",
    "        elif method == \"shot\":\n",
    "            method_name = f\"shot_r{r}\"\n",
    "        elif method == \"usc\":\n",
    "            method_name = f\"usc_r{r}\"\n",
    "        else:\n",
    "            method_name = method\n",
    "\n",
    "        in_path = f\"/path/to/inpath/{method_name}/\"\n",
    "        names = os.listdir(in_path)\n",
    "        \n",
    "        if len(names) == 48:\n",
    "        \n",
    "            distances_match =  []\n",
    "            scores_match = []\n",
    "            distances_mismatch = []\n",
    "            scores_mismatch = []\n",
    "            count = 0\n",
    "\n",
    "            for q_num, q_name in enumerate(names):\n",
    "                for t_num, t_name in enumerate(names):\n",
    "                    count += 1\n",
    "                    print(f\"Keypoint Correspondance: {round((count/(len(names)*len(names)))*100,2)}%        \", end = \"\\r\")\n",
    "            \n",
    "                    if t_num != q_num and len(q_name.split(\"part1\")) == 1 and len(t_name.split(\"part1\")) == 1:\n",
    "                    \n",
    "                        # Load data\n",
    "                        q_f = open(os.path.join(in_path,q_name),'rb')\n",
    "                        q_rep = json.load(q_f)\n",
    "                        q_rep = convert_to_array(q_rep)\n",
    "                        t_f = open(os.path.join(in_path,t_name),'rb')\n",
    "                        t_rep = json.load(t_f)\n",
    "                        t_rep = convert_to_array(t_rep)\n",
    "                    \n",
    "                        #Get difference\n",
    "                        distances, score, warning = landmark_correspondence(q_rep, t_rep, method=\"ratio\")\n",
    "                    \n",
    "                        #Check matches\n",
    "                        if str(q_name.split(\"_\")[:3]) == str(t_name.split(\"_\")[:3]):\n",
    "                            distances_match.append(distances)\n",
    "                            scores_match.append(score)\n",
    "                        else:\n",
    "                            distances_mismatch.append(distances)\n",
    "                            scores_mismatch.append(score)\n",
    "                        \n",
    "                        q_f.close()\n",
    "                        t_f.close()\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "            # Plot the distance  outcome\n",
    "            print(\"\\nMatches: \",len(distances_match))\n",
    "            print(\"Mismatches: \",len(distances_mismatch))\n",
    "            for red in distances_mismatch:\n",
    "                plt.hist(red, bins=100, histtype = 'step', lw=2, color=\"red\", alpha=0.2,density = True)\n",
    "            for blue in distances_match:\n",
    "                plt.hist(blue, bins=100, histtype = 'step', lw=2, color=\"cornflowerblue\", alpha=0.2,density = True)\n",
    "            plt.title(\"L2 Ratios    Method: \" + str(method_name))\n",
    "            plt.xlabel('Ratio of distances (closest/next closest)')\n",
    "            plt.ylabel('Histogram density')\n",
    "            red_patch = mpatches.Patch(color='red', label='Mismatch')\n",
    "            blue_patch = mpatches.Patch(color='cornflowerblue', label='Match')\n",
    "            plt.legend(handles=[red_patch, blue_patch],loc='upper left')\n",
    "            plt.savefig(os.path.join(in_path,f'hist_dist_{method_name}.pdf'))\n",
    "            plt.clf()\n",
    "            \n",
    "            # Plot the distance  outcome\n",
    "            mark50_match = []\n",
    "            mark50_mismatch = []\n",
    "            for red in distances_mismatch:\n",
    "                mismatch_cum = plt.hist(red, bins=100, histtype = 'step', lw=2, color=\"red\", alpha=0.2,density = True,cumulative=True)\n",
    "                mark50_mismatch.append(mismatch_cum[1][[ n for n,i in enumerate(mismatch_cum[0]) if i>=0.5 ][0]])\n",
    "            for blue in distances_match:\n",
    "                match_cum = plt.hist(blue, bins=100, histtype = 'step', lw=2, color=\"cornflowerblue\", alpha=0.2,density = True,cumulative=True)\n",
    "                mark50_match.append(match_cum[1][[ n for n,i in enumerate(match_cum[0]) if i>=0.5 ][0]])\n",
    "            plt.title(\"L2 Ratios    Method: \" + str(method_name))\n",
    "            plt.xlabel('Ratio of distances (closest/next closest) (cummulative)')\n",
    "            plt.ylabel('Cumulative histogram density')\n",
    "            red_patch = mpatches.Patch(color='red', label='Mismatch')\n",
    "            blue_patch = mpatches.Patch(color='cornflowerblue', label='Match')\n",
    "            plt.legend(handles=[red_patch, blue_patch],loc='upper left')\n",
    "            plt.savefig(os.path.join(in_path,f'hist_cum_dist_{method_name}.pdf'))\n",
    "            plt.clf()\n",
    "                \n",
    "            # Plot the scores outcome\n",
    "            q_bins = round((max(scores_match)-min(scores_match))*50)\n",
    "            plt.hist(scores_match,bins = q_bins, histtype = 'step', lw=2, color=\"cornflowerblue\", alpha=0.8)\n",
    "            plt.hist(scores_mismatch,bins = 10, histtype = 'step', lw=2, color=\"red\", alpha=0.8)\n",
    "            plt.title(\"IQRs    Method: \" + str(method_name))\n",
    "            plt.xlabel('IQR')\n",
    "            plt.ylabel('Counts')\n",
    "            red_patch = mpatches.Patch(color='red', label='Mismatch')\n",
    "            blue_patch = mpatches.Patch(color='cornflowerblue', label='Match')\n",
    "            plt.legend(handles=[red_patch, blue_patch],loc='upper right')\n",
    "            plt.savefig(os.path.join(in_path,f'hist_score_{method_name}.pdf'))\n",
    "            plt.clf()\n",
    "            \n",
    "            # Plot the scores outcome\n",
    "            q_bins = round((max(mark50_match)-min(mark50_match))*50)\n",
    "            plt.hist(mark50_match,bins = q_bins, histtype = 'step', lw=2, color=\"cornflowerblue\", alpha=0.8)\n",
    "            plt.hist(mark50_mismatch,bins = 10, histtype = 'step', lw=2, color=\"red\", alpha=0.8)\n",
    "            plt.title(\"Method: \" + str(method_name))\n",
    "            plt.xlabel('50% cumulative mark')\n",
    "            plt.ylabel('Counts')\n",
    "            red_patch = mpatches.Patch(color='red', label='Mismatch')\n",
    "            blue_patch = mpatches.Patch(color='cornflowerblue', label='Match')\n",
    "            plt.legend(handles=[red_patch, blue_patch],loc='upper left')\n",
    "            plt.savefig(os.path.join(in_path,f'hist_score_50_mark_{method_name}.pdf'))\n",
    "            plt.clf()\n",
    "            \n",
    "        else:\n",
    "            print(\"Not done calculating. Skip\")\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde9b1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd7fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0815779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c414d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0266b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3452a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a99b66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc711a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c1ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b27b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcbc667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyshot_kernel",
   "language": "python",
   "name": "pyshot_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
